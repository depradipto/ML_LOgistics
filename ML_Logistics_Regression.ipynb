{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "1. What is Logistic Regression, and how does it differ from Linear Regression?\n",
        " - Logistic Regression is a statistical method used for binary or multiclass classification, not regression. It predicts probabilities using the logistic (sigmoid) function.\n",
        "\n",
        "Linear Regression predicts continuous values using a straight line.\n",
        "\n",
        "Logistic Regression predicts class probabilities using a sigmoid curve and classifies based on a threshold (usually 0.5).\n",
        "\n",
        "2. What is the mathematical equation of Logistic Regression?\n",
        "ùëÉ(ùë¶=1‚à£ùë•)=1/1+ùëí‚àí(ùõΩ0+ùõΩ1ùë•1+ùõΩ2ùë•2+‚Ä¶+ùõΩùëõùë•ùëõ)P(y=1‚à£x)=1+e ‚àí(Œ≤0+Œ≤1x1+Œ≤2x2+‚Ä¶+Œ≤nxn)\n",
        "\n",
        "This is the sigmoid of the linear combination of inputs.\n",
        "\n",
        "3. Why do we use the Sigmoid function in Logistic Regression?\n",
        " - The sigmoid function maps any real-valued number into the range (0, 1). This makes it ideal for modeling probabilities, allowing us to classify outcomes based on a threshold.\n",
        "\n",
        "4. What is the cost function of Logistic Regression?\n",
        " - The cost function of logistic regression is a measure of the difference between the predicted probabilities and the actual labels. It's used to determine the loss or penalty for incorrect predictions. The cost function is typically defined as the negative log-likelihood of the data, given the model's parameters. In other words, it's the logarithm of the probability of observing the actual labels, given the model's predictions. This function is minimized during training to find the optimal parameters that result in the most accurate predictions. It's often referred to as the binary cross-entropy loss function.\n",
        "\n",
        "5. What is Regularization in Logistic Regression?\n",
        " - Regularization is a technique to prevent overfitting by adding a penalty term to the cost function, discouraging overly complex models (large coefficients).\n",
        "\n",
        "6. Why is it needed? Difference between Lasso, Ridge, and Elastic Net\n",
        " - The choice of Lasso, Ridge, or Elastic Net regression depends on the nature of the data and the specific problem being solved. Lasso (Least Absolute Shrinkage Operator) is suitable for high-dimensional data with correlated features, as it shrinks or eliminates irrelevant features. Ridge regression is used when there are highly correlated features, as it adds a penalty to the coefficients but keeps them non-zero. Elastic Net is a combination of Lasso and Ridge, suitable for cases with both high dimensionality and correlated features. The choice between these methods helps prevent overfitting and identify relevant features.\n",
        "\n",
        " 7. When should we use Elastic Net instead of Lasso or Ridge?\n",
        " - Use Elastic Net when:\n",
        "\n",
        "You have many correlated features.\n",
        "\n",
        "You want both feature selection (L1) and stability (L2).\n",
        "\n",
        "Lasso alone is unstable or removes too many features.\n",
        "\n",
        "8. What is the impact of the regularization parameter (Œª)?\n",
        " - Large Œª: Strong regularization, smaller coefficients, possibly underfitting.\n",
        "\n",
        "Small Œª: Weak regularization, allows large coefficients, may overfit.\n",
        "\n",
        "Œª = 0: No regularization (pure logistic regression).\n",
        "\n",
        "9. What are the key assumptions of Logistic Regression?\n",
        " - No multicollinearity among predictors.\n",
        "\n",
        "Linear relationship between features and log-odds.\n",
        "\n",
        "Observations are independent.\n",
        "\n",
        "Large sample size for reliable estimates.\n",
        "\n",
        "10. What are some alternatives to Logistic Regression for classification tasks?\n",
        " - Decision Trees\n",
        "\n",
        "Random Forests\n",
        "\n",
        "Support Vector Machines (SVM)\n",
        "\n",
        "K-Nearest Neighbors (KNN)\n",
        "\n",
        "Naive Bayes\n",
        "\n",
        "Gradient Boosting (XGBoost, LightGBM)\n",
        "\n",
        "Neural Networks\n",
        "\n",
        "11. What are Classification Evaluation Metrics?\n",
        " - Accuracy\n",
        "\n",
        "Precision\n",
        "\n",
        "Recall\n",
        "\n",
        "F1-Score\n",
        "\n",
        "ROC-AUC\n",
        "\n",
        "Confusion Matrix\n",
        "\n",
        "Matthews Correlation Coefficient (MCC)\n",
        "\n",
        "Cohen‚Äôs Kappa\n",
        "\n",
        "Precision-Recall Curve\n",
        "\n",
        "12. How does class imbalance affect Logistic Regression?\n",
        " - The model may be biased toward the majority class.\n",
        "\n",
        "Metrics like accuracy become misleading.\n",
        "\n",
        "Solutions:\n",
        "\n",
        "Class weights\n",
        "\n",
        "Resampling (SMOTE, undersampling)\n",
        "\n",
        "Use metrics like F1-score, ROC-AUC, not just accuracy.\n",
        "\n",
        "13. What is Hyperparameter Tuning in Logistic Regression?\n",
        " - It's the process of finding the best values for parameters like:\n",
        "\n",
        "C (inverse of Œª, regularization strength)\n",
        "\n",
        "penalty (L1, L2, elasticnet)\n",
        "\n",
        "solver (optimization algorithm)\n",
        "\n",
        "Tuning is done using:\n",
        "\n",
        "GridSearchCV\n",
        "\n",
        "RandomizedSearchCV\n",
        "\n",
        "14. What are different solvers in Logistic Regression?\n",
        " - liblinear: Good for small datasets, supports L1.\n",
        "\n",
        "lbfgs: Fast and works well for multiclass and L2.\n",
        "\n",
        "saga: Supports L1, L2, elasticnet, good for large datasets.\n",
        "\n",
        "newton-cg: For L2 regularization.\n",
        "\n",
        "15. Which solver should be used? How is Logistic Regression extended for multiclass?\n",
        " - liblinear: Binary or OvR with L1/L2.\n",
        "\n",
        "lbfgs/saga: Best for multiclass, large data.\n",
        "\n",
        "Multiclass extensions:\n",
        "\n",
        "One-vs-Rest (OvR): Train one classifier per class.\n",
        "\n",
        "Softmax (Multinomial Logistic Regression): Single model predicting all classes.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        " 16. What are the advantages and disadvantages of Logistic Regression?\n",
        " - Advantages:\n",
        "\n",
        "Simple and fast\n",
        "\n",
        "Works well with linearly separable data\n",
        "\n",
        "Interpretable coefficients\n",
        "\n",
        " - Disadvantages:\n",
        "\n",
        "Assumes linear boundary in log-odds\n",
        "\n",
        "Poor with complex relationships\n",
        "\n",
        "Sensitive to outliers and multicollinearity\n",
        "\n",
        "17. What are some use cases of Logistic RegressionEmail spam detection\n",
        "\n",
        " - Credit default prediction\n",
        "\n",
        "Customer churn\n",
        "\n",
        "Medical diagnosis (disease prediction)\n",
        "\n",
        "Fraud detection\n",
        "\n",
        "Marketing response prediction\n",
        "\n",
        "18. What is the difference between Softmax Regression and Logistic Regression\n",
        " - Logistic Regression: Binary classification.\n",
        "\n",
        "Softmax Regression: Multiclass extension, outputs probabilities for all classes using the softmax function.\n",
        "\n",
        "19. How do we choose between One-vs-Rest (OvR) and Softmax for multiclass classification?\n",
        "\n",
        " - Use OvR when:\n",
        "\n",
        "Classes are imbalanced\n",
        "\n",
        "Simpler implementation works\n",
        "\n",
        "Use Softmax when:\n",
        "\n",
        "Classes are mutually exclusive\n",
        "\n",
        "You want direct multiclass probability predictions\n",
        "\n",
        "20. How to Interpret Coefficients in Logistic Regression?\n",
        " - Each coefficient represents the change in the log-odds of the outcome for a one-unit increase in that predictor, holding others constant.\n",
        "\n",
        "Positive coefficient ‚Üí increases probability of positive class.\n",
        "\n",
        "Negative coefficient ‚Üí decreases it.\n",
        "\n",
        "e^Œ≤i gives the odds ratio.\n",
        "\n"
      ],
      "metadata": {
        "id": "DP_JvzTMkit_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#1. Write a Python program that loads a dataset, splits it into training and testing sets, applies Logistic Regression, and prints the model accuracy\n",
        "\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load dataset\n",
        "data = load_breast_cancer()\n",
        "X, y = data.data, data.target\n",
        "\n",
        "# Split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train Logistic Regression\n",
        "model = LogisticRegression(max_iter=1000)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Predict and evaluate\n",
        "y_pred = model.predict(X_test)\n",
        "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "en7oFojQlQK-",
        "outputId": "46386d54-80c9-482a-a8ea-46e2321da811"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.956140350877193\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#2. Write a Python program to apply L1 regularization (Lasso) on a dataset using LogisticRegression(penalty='l1') and print the model accuracy\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "model = LogisticRegression(penalty='l1', solver='liblinear', max_iter=1000)\n",
        "model.fit(X_train, y_train)\n",
        "y_pred = model.predict(X_test)\n",
        "print(\"L1 Regularization Accuracy:\", accuracy_score(y_test, y_pred))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ovJ2yIApoYoJ",
        "outputId": "1a501c90-35de-4c04-88f1-1e54d510d930"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "L1 Regularization Accuracy: 0.956140350877193\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#3. Write a Python program to train Logistic Regression with L2 regularization (Ridge) using LogisticRegression(penalty='l2'). Print model accuracy and coefficients\n",
        "model = LogisticRegression(penalty='l2', solver='liblinear', max_iter=1000)\n",
        "model.fit(X_train, y_train)\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "print(\"L2 Regularization Accuracy:\", accuracy_score(y_test, y_pred))\n",
        "print(\"Coefficients:\", model.coef_)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yfEIaWopnnZv",
        "outputId": "57a71480-7e10-4654-f682-80fa2511ca6d"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "L2 Regularization Accuracy: 0.956140350877193\n",
            "Coefficients: [[ 2.13248406e+00  1.52771940e-01 -1.45091255e-01 -8.28669349e-04\n",
            "  -1.42636015e-01 -4.15568847e-01 -6.51940282e-01 -3.44456106e-01\n",
            "  -2.07613380e-01 -2.97739324e-02 -5.00338038e-02  1.44298427e+00\n",
            "  -3.03857384e-01 -7.25692126e-02 -1.61591524e-02 -1.90655332e-03\n",
            "  -4.48855442e-02 -3.77188737e-02 -4.17516190e-02  5.61347410e-03\n",
            "   1.23214996e+00 -4.04581097e-01 -3.62091502e-02 -2.70867580e-02\n",
            "  -2.62630530e-01 -1.20898539e+00 -1.61796947e+00 -6.15250835e-01\n",
            "  -7.42763610e-01 -1.16960181e-01]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#4. Write a Python program to train Logistic Regression with Elastic Net Regularization (penalty='elasticnet')\n",
        "model = LogisticRegression(penalty='elasticnet', solver='saga', l1_ratio=0.5, max_iter=1000)\n",
        "model.fit(X_train, y_train)\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "print(\"Elastic Net Accuracy:\", accuracy_score(y_test, y_pred))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HToJyoIFnnWg",
        "outputId": "558921ec-1952-4062-8fa9-e30bf55f3a62"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Elastic Net Accuracy: 1.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#5. Write a Python program to train a Logistic Regression model for multiclass classification using multi_class='ovr'.\n",
        "from sklearn.datasets import load_iris\n",
        "\n",
        "# Load multiclass data\n",
        "iris = load_iris()\n",
        "X, y = iris.data, iris.target\n",
        "\n",
        "# Split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train OvR model\n",
        "model = LogisticRegression(multi_class='ovr', solver='liblinear', max_iter=1000)\n",
        "model.fit(X_train, y_train)\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "print(\"Multiclass OvR Accuracy:\", accuracy_score(y_test, y_pred))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dZB6kWz3nnS3",
        "outputId": "6fb81133-aa15-48a3-f643-fcadbac24946"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Multiclass OvR Accuracy: 1.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1256: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. Use OneVsRestClassifier(LogisticRegression(..)) instead. Leave it to its default value to avoid this warning.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#6. Write a Python program to apply GridSearchCV to tune the hyperparameters (C and penalty) of Logistic Regression. Print the best parameters and accuracy\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "param_grid = {\n",
        "    'C': [0.01, 0.1, 1, 10],\n",
        "    'penalty': ['l1', 'l2'],\n",
        "    'solver': ['liblinear']\n",
        "}\n",
        "\n",
        "grid = GridSearchCV(LogisticRegression(max_iter=1000), param_grid, cv=5)\n",
        "grid.fit(X_train, y_train)\n",
        "\n",
        "print(\"Best Parameters:\", grid.best_params_)\n",
        "print(\"Best Accuracy:\", grid.best_score_)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AXPK0pFYnnPI",
        "outputId": "88cb4304-f0df-45bc-93fc-a7c4136816ca"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best Parameters: {'C': 10, 'penalty': 'l1', 'solver': 'liblinear'}\n",
            "Best Accuracy: 0.9583333333333334\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#7. Write a Python program to evaluate Logistic Regression using Stratified K-Fold Cross-Validation. Print the average accuracy.\n",
        "from sklearn.model_selection import StratifiedKFold, cross_val_score\n",
        "\n",
        "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
        "model = LogisticRegression(max_iter=1000)\n",
        "\n",
        "scores = cross_val_score(model, X, y, cv=skf, scoring='accuracy')\n",
        "print(\"Stratified K-Fold Accuracy Scores:\", scores)\n",
        "print(\"Average Accuracy:\", scores.mean())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x4UJhp4tnnLW",
        "outputId": "e9064161-9403-4b6b-a817-282b7923f184"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Stratified K-Fold Accuracy Scores: [1.         0.96666667 0.93333333 1.         0.93333333]\n",
            "Average Accuracy: 0.9666666666666668\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#8. Write a Python program to load a dataset from a CSV file, apply Logistic Regression, and evaluate its accuracy.\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load the dataset\n",
        "df = pd.read_csv(\"health_activity_data.csv\")\n",
        "\n",
        "# Drop non-numeric or irrelevant columns\n",
        "df_clean = df.drop(['ID', 'Blood_Pressure'], axis=1)\n",
        "\n",
        "# Convert categorical variables to dummy/indicator variables\n",
        "df_encoded = pd.get_dummies(df_clean, drop_first=True)\n",
        "\n",
        "# Define features (X) and target (y)\n",
        "X = df_encoded.drop('Heart_Disease_Yes', axis=1)\n",
        "y = df_encoded['Heart_Disease_Yes']\n",
        "\n",
        "# Split into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Initialize and train the Logistic Regression model\n",
        "model = LogisticRegression(max_iter=1000)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Evaluate and print accuracy\n",
        "accuracy = model.score(X_test, y_test)\n",
        "print(\"Model Accuracy:\", accuracy)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ssglRVdfnnH7",
        "outputId": "a4ff7509-bcc9-456f-b174-0247cc1525c5"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model Accuracy: 0.93\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#9. Write a Python program to apply RandomizedSearchCV for tuning hyperparameters (C, penalty, solver) in Logistic Regression. Print the best parameters and accuracy.\n",
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "import numpy as np\n",
        "\n",
        "param_dist = {\n",
        "    'C': np.logspace(-3, 2, 10),\n",
        "    'penalty': ['l1', 'l2'],\n",
        "    'solver': ['liblinear', 'saga']\n",
        "}\n",
        "\n",
        "rand_search = RandomizedSearchCV(LogisticRegression(max_iter=1000), param_dist, n_iter=10, cv=5, random_state=42)\n",
        "rand_search.fit(X_train, y_train)\n",
        "\n",
        "print(\"Best Parameters:\", rand_search.best_params_)\n",
        "print(\"Best Accuracy:\", rand_search.best_score_)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WjvZdzqynnEA",
        "outputId": "29252cef-c37f-4f45-bb9b-a5c00da2f4fd"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best Parameters: {'solver': 'saga', 'penalty': 'l2', 'C': np.float64(0.1668100537200059)}\n",
            "Best Accuracy: 0.9012499999999999\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#10. Write a Python program to implement One-vs-One (OvO) Multiclass Logistic Regression and print accuracy.\n",
        "from sklearn.multiclass import OneVsOneClassifier\n",
        "\n",
        "ovo_model = OneVsOneClassifier(LogisticRegression(max_iter=1000))\n",
        "ovo_model.fit(X_train, y_train)\n",
        "y_pred = ovo_model.predict(X_test)\n",
        "\n",
        "print(\"OvO Multiclass Accuracy:\", accuracy_score(y_test, y_pred))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1xBmZpfYnnAL",
        "outputId": "4c46874e-09d1-4dac-991a-9de2d7fbc20f"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "OvO Multiclass Accuracy: 0.93\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#11. Write a Python program to train a Logistic Regression model and visualize the confusion matrix for binary classification.\n",
        "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "model = LogisticRegression(max_iter=1000)\n",
        "model.fit(X_train, y_train)\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "disp = ConfusionMatrixDisplay(confusion_matrix=cm)\n",
        "disp.plot()\n",
        "plt.title(\"Confusion Matrix\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 611
        },
        "id": "wKvKhFwynm8f",
        "outputId": "f9ffffba-9346-4a4d-c1ed-5608d5e5f455"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 2 Axes>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAfsAAAHHCAYAAAC4M/EEAAAAOnRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjEwLjAsIGh0dHBzOi8vbWF0cGxvdGxpYi5vcmcvlHJYcgAAAAlwSFlzAAAPYQAAD2EBqD+naQAAPr1JREFUeJzt3XlclOX+//H3oDKAMiBuQCJuueVuxTHL5aupWGZZpzQ7obnUKaujWeYpFW2xn5aaZdpiah7N6lSWVpZLbkfypEa2qAniUoKapAgGAnP//jDmNOLCODMMM/fr+Xjcj4f3dW+fm8wPn+u67vu2GIZhCAAABKwgXwcAAAC8i2QPAECAI9kDABDgSPYAAAQ4kj0AAAGOZA8AQIAj2QMAEOBI9gAABDiSPQAAAY5kD5xlz5496tmzpyIiImSxWLRs2TKPnn/fvn2yWCxasGCBR8/rz7p27aquXbv6OgwgYJHsUSGlp6fr3nvvVcOGDRUSEiKbzaZOnTrpxRdf1O+//+7VayclJem7777TM888o0WLFunKK6/06vXK0+DBg2WxWGSz2c75c9yzZ48sFossFouef/55l89/6NAhJScnKzU11QPRAvCUyr4OADjbJ598or/+9a+yWq26++671bJlS50+fVqbNm3So48+qh9++EGvvfaaV679+++/KyUlRU888YRGjhzplWvEx8fr999/V5UqVbxy/oupXLmyTp06peXLl+v222932rZ48WKFhIQoPz//ks596NAhTZo0SfXr11fbtm3LfNwXX3xxSdcDUDYke1QoGRkZGjBggOLj47V27VrFxMQ4tj3wwANKS0vTJ5984rXrHz16VJIUGRnptWtYLBaFhIR47fwXY7Va1alTJ7399tulkv2SJUt0ww036P333y+XWE6dOqWwsDAFBweXy/UAs6IbHxXK1KlTlZubq3nz5jkl+hKNGzfWww8/7FgvKirSU089pUaNGslqtap+/fr65z//qYKCAqfj6tevrxtvvFGbNm3S1VdfrZCQEDVs2FBvvfWWY5/k5GTFx8dLkh599FFZLBbVr19f0pnu75I//1lycrIsFotT26pVq3TttdcqMjJS1apVU9OmTfXPf/7Tsf18Y/Zr167Vddddp6pVqyoyMlL9+vXTzp07z3m9tLQ0DR48WJGRkYqIiNCQIUN06tSp8/9gz3LnnXfqs88+0/Hjxx1tX3/9tfbs2aM777yz1P7Z2dkaM2aMWrVqpWrVqslmsykxMVHffvutY59169bpqquukiQNGTLEMRxQcp9du3ZVy5YttW3bNnXu3FlhYWGOn8vZY/ZJSUkKCQkpdf+9evVS9erVdejQoTLfKwCSPSqY5cuXq2HDhrrmmmvKtP+wYcM0YcIEtW/fXjNmzFCXLl00ZcoUDRgwoNS+aWlpuu2223T99dfrhRdeUPXq1TV48GD98MMPkqT+/ftrxowZkqSBAwdq0aJFmjlzpkvx//DDD7rxxhtVUFCgyZMn64UXXtBNN92k//znPxc8bvXq1erVq5eOHDmi5ORkjR49Wps3b1anTp20b9++UvvffvvtOnnypKZMmaLbb79dCxYs0KRJk8ocZ//+/WWxWPTBBx842pYsWaJmzZqpffv2pfbfu3evli1bphtvvFHTp0/Xo48+qu+++05dunRxJN7mzZtr8uTJkqQRI0Zo0aJFWrRokTp37uw4z7Fjx5SYmKi2bdtq5syZ6tat2znje/HFF1WrVi0lJSWpuLhYkvTqq6/qiy++0EsvvaTY2Ngy3ysASQZQQZw4ccKQZPTr169M+6emphqSjGHDhjm1jxkzxpBkrF271tEWHx9vSDI2bNjgaDty5IhhtVqNRx55xNGWkZFhSDKmTZvmdM6kpCQjPj6+VAwTJ040/vy/0YwZMwxJxtGjR88bd8k15s+f72hr27atUbt2bePYsWOOtm+//dYICgoy7r777lLXu+eee5zOecsttxg1atQ47zX/fB9Vq1Y1DMMwbrvtNqN79+6GYRhGcXGxER0dbUyaNOmcP4P8/HyjuLi41H1YrVZj8uTJjravv/661L2V6NKliyHJmDt37jm3denSxant888/NyQZTz/9tLF3716jWrVqxs0333zRewRQGpU9KoycnBxJUnh4eJn2//TTTyVJo0ePdmp/5JFHJKnU2H6LFi103XXXOdZr1aqlpk2bau/evZcc89lKxvo/+ugj2e32Mh2TmZmp1NRUDR48WFFRUY721q1b6/rrr3fc55/dd999TuvXXXedjh075vgZlsWdd96pdevWKSsrS2vXrlVWVtY5u/ClM+P8QUFn/rkoLi7WsWPHHEMU27dvL/M1rVarhgwZUqZ9e/bsqXvvvVeTJ09W//79FRISoldffbXM1wLwPyR7VBg2m02SdPLkyTLtv3//fgUFBalx48ZO7dHR0YqMjNT+/fud2uvVq1fqHNWrV9dvv/12iRGXdscdd6hTp04aNmyY6tSpowEDBujdd9+9YOIvibNp06altjVv3ly//vqr8vLynNrPvpfq1atLkkv30qdPH4WHh+udd97R4sWLddVVV5X6WZaw2+2aMWOGLr/8clmtVtWsWVO1atXSjh07dOLEiTJf87LLLnNpMt7zzz+vqKgopaamatasWapdu3aZjwXwPyR7VBg2m02xsbH6/vvvXTru7Aly51OpUqVzthuGccnXKBlPLhEaGqoNGzZo9erV+tvf/qYdO3bojjvu0PXXX19qX3e4cy8lrFar+vfvr4ULF+rDDz88b1UvSc8++6xGjx6tzp0761//+pc+//xzrVq1SldccUWZezCkMz8fV3zzzTc6cuSIJOm7775z6VgA/0OyR4Vy4403Kj09XSkpKRfdNz4+Xna7XXv27HFqP3z4sI4fP+6YWe8J1atXd5q5XuLs3gNJCgoKUvfu3TV9+nT9+OOPeuaZZ7R27Vp9+eWX5zx3SZy7d+8utW3Xrl2qWbOmqlat6t4NnMedd96pb775RidPnjznpMYS//73v9WtWzfNmzdPAwYMUM+ePdWjR49SP5Oy/uJVFnl5eRoyZIhatGihESNGaOrUqfr66689dn7ATEj2qFAee+wxVa1aVcOGDdPhw4dLbU9PT9eLL74o6Uw3tKRSM+anT58uSbrhhhs8FlejRo104sQJ7dixw9GWmZmpDz/80Gm/7OzsUseWvFzm7McBS8TExKht27ZauHChU/L8/vvv9cUXXzju0xu6deump556Si+//LKio6PPu1+lSpVK9Rq89957+uWXX5zaSn4pOdcvRq4aO3asDhw4oIULF2r69OmqX7++kpKSzvtzBHB+vFQHFUqjRo20ZMkS3XHHHWrevLnTG/Q2b96s9957T4MHD5YktWnTRklJSXrttdd0/PhxdenSRf/973+1cOFC3Xzzzed9rOtSDBgwQGPHjtUtt9yihx56SKdOndKcOXPUpEkTpwlqkydP1oYNG3TDDTcoPj5eR44c0SuvvKK6devq2muvPe/5p02bpsTERHXs2FFDhw7V77//rpdeekkRERFKTk722H2cLSgoSE8++eRF97vxxhs1efJkDRkyRNdcc42+++47LV68WA0bNnTar1GjRoqMjNTcuXMVHh6uqlWrKiEhQQ0aNHAprrVr1+qVV17RxIkTHY8Czp8/X127dtX48eM1depUl84HmJ6PnwYAzumnn34yhg8fbtSvX98IDg42wsPDjU6dOhkvvfSSkZ+f79ivsLDQmDRpktGgQQOjSpUqRlxcnDFu3DinfQzjzKN3N9xwQ6nrnP3I1/kevTMMw/jiiy+Mli1bGsHBwUbTpk2Nf/3rX6UevVuzZo3Rr18/IzY21ggODjZiY2ONgQMHGj/99FOpa5z9eNrq1auNTp06GaGhoYbNZjP69u1r/Pjjj077lFzv7Ef75s+fb0gyMjIyzvszNQznR+/O53yP3j3yyCNGTEyMERoaanTq1MlISUk55yNzH330kdGiRQujcuXKTvfZpUsX44orrjjnNf98npycHCM+Pt5o3769UVhY6LTfqFGjjKCgICMlJeWC9wDAmcUwXJjRAwAA/A5j9gAABDiSPQAAAY5kDwBAgCPZAwAQ4Ej2AAAEOJI9AAABzq9fqmO323Xo0CGFh4d79DWdAIDyYRiGTp48qdjYWMeXFb0hPz9fp0+fdvs8wcHBCgkJ8UBE5cuvk/2hQ4cUFxfn6zAAAG46ePCg6tat65Vz5+fnq0F8NWUdcf9jVNHR0crIyPC7hO/Xyb7ku+f7t9eXrRojEghMtzRp5esQAK8pUqE26VPHv+fecPr0aWUdKdb+bfVlC7/0XJFz0q74Dvt0+vRpkn15Kum6t1ULcus/IFCRVbZU8XUIgPf88Q7X8hiKrRZuUbXwS7+OXf47XOzXyR4AgLIqNuwqduMF8cWG3XPBlDOSPQDAFOwyZNelZ3t3jvU1+r4BAAhwVPYAAFOwyy53OuLdO9q3SPYAAFMoNgwVu/FVd3eO9TW68QEACHBU9gAAUzDzBD2SPQDAFOwyVGzSZE83PgAAAY7KHgBgCnTjAwAQ4JiNDwAAAhaVPQDAFOx/LO4c769I9gAAUyh2cza+O8f6GskeAGAKxYbc/Oqd52Ipb4zZAwDgBRs2bFDfvn0VGxsri8WiZcuWOW23WCznXKZNm+bYp379+qW2P/fccy7HQmUPADCF8h6zz8vLU5s2bXTPPfeof//+pbZnZmY6rX/22WcaOnSobr31Vqf2yZMna/jw4Y718PBwFyMh2QMATMIui4plcet4VyQmJioxMfG826Ojo53WP/roI3Xr1k0NGzZ0ag8PDy+1r6voxgcAwAU5OTlOS0FBgdvnPHz4sD755BMNHTq01LbnnntONWrUULt27TRt2jQVFRW5fH4qewCAKdiNM4s7x0tSXFycU/vEiROVnJx86SeWtHDhQoWHh5fq7n/ooYfUvn17RUVFafPmzRo3bpwyMzM1ffp0l85PsgcAmEKxm934JccePHhQNpvN0W61Wt2O7c0339SgQYMUEhLi1D569GjHn1u3bq3g4GDde++9mjJlikvXJdkDAOACm83mlOzdtXHjRu3evVvvvPPORfdNSEhQUVGR9u3bp6ZNm5b5GiR7AIApeKqy97R58+apQ4cOatOmzUX3TU1NVVBQkGrXru3SNUj2AABTsBsW2Q03ZuO7eGxubq7S0tIc6xkZGUpNTVVUVJTq1asn6cxkv/fee08vvPBCqeNTUlK0ZcsWdevWTeHh4UpJSdGoUaN01113qXr16i7FQrIHAMALtm7dqm7dujnWS8bfk5KStGDBAknS0qVLZRiGBg4cWOp4q9WqpUuXKjk5WQUFBWrQoIFGjRrlNI5fVhbD8N9v9uXk5CgiIkK//dRQtnCeIkRg6hXb1tchAF5TZBRqnT7SiRMnPDoO/mcluWL995epmhu5IvekXV1a/uLVWL2Fyh4AYArFClKxG6+XKfZgLOWNZA8AMAXDzTF7w41jfY2+bwAAAhyVPQDAFCrqo3flgWQPADCFYiNIxYYbY/Z+O52dbnwAAAIelT0AwBTsssjuRo1rl/+W9iR7AIApmHnMnm58AAACHJU9AMAU3J+gRzc+AAAV2pkxezc+hEM3PgAAqKio7AEApmB38934zMYHAKCCY8weAIAAZ1eQaZ+zZ8weAIAAR2UPADCFYsOiYjc+U+vOsb5GsgcAmEKxmxP0iunGBwAAFRWVPQDAFOxGkOxuzMa3MxsfAICKjW58AAAQsKjsAQCmYJd7M+rtngul3JHsAQCm4P5Ldfy3M9x/IwcAAGVCZQ8AMAX3343vv/UxyR4AYApm/p49yR4AYApmruz9N3IAAFAmVPYAAFNw/6U6/lsfk+wBAKZgNyyyu/OcvR9/9c5/f00BAABlQmUPADAFu5vd+P78Uh2SPQDAFNz/6p3/Jnv/jRwAAJQJlT0AwBSKZVGxGy/GcedYXyPZAwBMgW58AAAQsEj2AABTKNb/uvIvbXHNhg0b1LdvX8XGxspisWjZsmVO2wcPHiyLxeK09O7d22mf7OxsDRo0SDabTZGRkRo6dKhyc3NdvneSPQDAFEq68d1ZXJGXl6c2bdpo9uzZ592nd+/eyszMdCxvv/220/ZBgwbphx9+0KpVq7RixQpt2LBBI0aMcPneGbMHAJhCeX8IJzExUYmJiRfcx2q1Kjo6+pzbdu7cqZUrV+rrr7/WlVdeKUl66aWX1KdPHz3//POKjY0tcyxU9gAA+Mi6detUu3ZtNW3aVH//+9917Ngxx7aUlBRFRkY6Er0k9ejRQ0FBQdqyZYtL16GyBwCYguHm9+yNP47NyclxardarbJarS6fr3fv3urfv78aNGig9PR0/fOf/1RiYqJSUlJUqVIlZWVlqXbt2k7HVK5cWVFRUcrKynLpWiR7AIApeKobPy4uzql94sSJSk5Odvl8AwYMcPy5VatWat26tRo1aqR169ape/fulxznuZDsAQBwwcGDB2Wz2Rzrl1LVn0vDhg1Vs2ZNpaWlqXv37oqOjtaRI0ec9ikqKlJ2dvZ5x/nPh2QPADAFT33i1mazOSV7T/n555917NgxxcTESJI6duyo48ePa9u2berQoYMkae3atbLb7UpISHDp3CR7AIApFLv51TtXj83NzVVaWppjPSMjQ6mpqYqKilJUVJQmTZqkW2+9VdHR0UpPT9djjz2mxo0bq1evXpKk5s2bq3fv3ho+fLjmzp2rwsJCjRw5UgMGDHBpJr7EbHwAALxi69atateundq1aydJGj16tNq1a6cJEyaoUqVK2rFjh2666SY1adJEQ4cOVYcOHbRx40anYYHFixerWbNm6t69u/r06aNrr71Wr732msuxUNkDAEzBU934ZdW1a1cZhnHe7Z9//vlFzxEVFaUlS5a4dN1zIdkDAEzBriDZ3ejQdudYX/PfyAEAQJlQ2QMATKHYsKjYjW58d471NZI9AMAUynvMviIh2QMATMG4hC/XnX28v/LfyAEAQJlQ2QMATKFYFhW78SEcd471NZI9AMAU7IZ74+728z8yX+HRjQ8AQICjsoe++6qq3nultvZ8F6bsw1U0cV6Grkk84dj+e16Q5j0To5TPI5TzW2VFx51Wv6FHdePdx5zO8+PWMC34fzHatT1MlSpJDa/4Xc8uSZc11I9/HYap9B38q277+xFF1SrS3h9D9cqTl2l3apivw4KH2N2coOfOsb5WISKfPXu26tevr5CQECUkJOi///2vr0MylfxTQWp4xe8a+ezP59z+anKstq6z6bGXDuj19bt0y/Cjmv1EXaV8/r+vPv24NUxPDGqkDp1PatanezTr059005BfZakQf8OAi+ty028aMfGQFk+P1gO9mmjvjyF6ZsleRdQo9HVo8BC7LG4v/srn/xS/8847Gj16tCZOnKjt27erTZs26tWrV6lv+MJ7rvq/kxo8Nkud/lTN/9mPW6vq+r9mq801uYqOO60+dx1Twxa/O1U8ryZfppuHHtUdDx5R/ab5imtcoC43HVewlaoe/qH/iF+1ckmUvngnSgf2hGjW2Loq+N2iXgOzfR0a4DafJ/vp06dr+PDhGjJkiFq0aKG5c+cqLCxMb775pq9Dwx9aXJmnr76I0K+ZVWQYUup/qumXvVZ16HJSknT818ratb2qImsU6R99L9cdra/QmP6N9f2Wqj6OHCibylXsurz1KW3fGO5oMwyLvtkYrhYdTvkwMnhSyRv03Fn8lU+T/enTp7Vt2zb16NHD0RYUFKQePXooJSXFh5Hhz+5/+hfVa5KvQR2u0A3xbfTkoIZ64Nmf1eoveZKkzP3BkqRF06OVOOiYnlm8V41bndLjdzTSL3uDfRk6UCa2qGJVqiwdP+o8jem3Xyureq0iH0UFTysZs3dn8Vc+naD366+/qri4WHXq1HFqr1Onjnbt2lVq/4KCAhUUFDjWc3JyvB4jpI/erKld28I0acFe1a57Wt99VU2z/1lXNeoUqn3nXNntZ/brc9cx9RpwpsuzcavflbopXJ8vraF7/pnpw+gBAH41G3/KlCmaNGmSr8MwlYLfLVrwXIwmzNunhB5nfrlq2CJfe38I1b/n1lb7zrmqUedM5RPfJN/p2LjG+TryS5VyjxlwVU52JRUXSZFnVfHVaxbpt6N+9c8kLsAuN9+NzwS9S1OzZk1VqlRJhw8fdmo/fPiwoqOjS+0/btw4nThxwrEcPHiwvEI1raIii4oKgxQU5DzRLqiSIeOPir5O3GnViD6tn9OtTvv8steq2nWZyYyKr6gwSHt2hKndtScdbRaLobbX5urHbTx6FygMN2fiGyT7SxMcHKwOHTpozZo1jja73a41a9aoY8eOpfa3Wq2y2WxOC9z3e16Q0r8PVfr3oZKkrIPBSv8+VEd+rqKq4Xa17pir15+K1bebqynrQLC+eCdKq/8d5XgW32KRbvv7US2bV0sbV0Tol4xgLZwarYPpIeo98NiFLg1UGB+8VlOJd2arx1+zFdc4Xw8+97NCwuz6YmmUr0ODh5R89c6dxV/5vH9q9OjRSkpK0pVXXqmrr75aM2fOVF5enoYMGeLr0Ezjp2/D9NhtjR3rryZfJkm6/vZsjZl5QOPm7NObz8bo/42sp5PHK6v2Zac1eGym00t1+g8/qsJ8i+ZOvEwnj1dSwxb5mvJ2umLrny73+wEuxfqPqyuiRrHufjRL1WsVae8PoXpiUAMd/5WhKPg/nyf7O+64Q0ePHtWECROUlZWltm3bauXKlaUm7cF72lyTq88PpZ53e1TtIo2ZefEhkzsePKI7HuT9CPBfH8+vqY/n1/R1GPASM79Bz+fJXpJGjhypkSNH+joMAEAAc7cr3p+78f331xQAAFAmFaKyBwDA29x9v70/P3pHsgcAmALd+AAAIGBR2QMATMHMlT3JHgBgCmZO9nTjAwAQ4KjsAQCmYObKnmQPADAFQ+49PmdcfJcKi2QPADAFM1f2jNkDABDgqOwBAKZg5sqeZA8AMAUzJ3u68QEACHBU9gAAUzBzZU+yBwCYgmFYZLiRsN051tfoxgcAIMBR2QMATMHM37OnsgcAmELJmL07iys2bNigvn37KjY2VhaLRcuWLXNsKyws1NixY9WqVStVrVpVsbGxuvvuu3Xo0CGnc9SvX18Wi8Vpee6551y+d5I9AABekJeXpzZt2mj27Nmltp06dUrbt2/X+PHjtX37dn3wwQfavXu3brrpplL7Tp48WZmZmY7lwQcfdDkWuvEBAKZQ3hP0EhMTlZiYeM5tERERWrVqlVPbyy+/rKuvvloHDhxQvXr1HO3h4eGKjo52PeA/obIHAJiCp7rxc3JynJaCggKPxHfixAlZLBZFRkY6tT/33HOqUaOG2rVrp2nTpqmoqMjlc1PZAwBMwVOVfVxcnFP7xIkTlZyc7E5oys/P19ixYzVw4EDZbDZH+0MPPaT27dsrKipKmzdv1rhx45SZmanp06e7dH6SPQAALjh48KBTQrZarW6dr7CwULfffrsMw9CcOXOcto0ePdrx59atWys4OFj33nuvpkyZ4tJ1SfYAAFMw3HyDXkllb7PZnJK9O0oS/f79+7V27dqLnjchIUFFRUXat2+fmjZtWubrkOwBAKZgSDIM9473pJJEv2fPHn355ZeqUaPGRY9JTU1VUFCQateu7dK1SPYAAHhBbm6u0tLSHOsZGRlKTU1VVFSUYmJidNttt2n79u1asWKFiouLlZWVJUmKiopScHCwUlJStGXLFnXr1k3h4eFKSUnRqFGjdNddd6l69eouxUKyBwCYgl0WWcrxDXpbt25Vt27dHOsl4+9JSUlKTk7Wxx9/LElq27at03FffvmlunbtKqvVqqVLlyo5OVkFBQVq0KCBRo0a5TSOX1YkewCAKZT3c/Zdu3aVcYFxgwttk6T27dvrq6++cuma58Nz9gAABDgqewCAKdgNiyx8zx4AgMBlGG7Oxvf0dPxyRDc+AAABjsoeAGAK5T1BryIh2QMATIFkDwBAgDPzBD3G7AEACHBU9gAAUzDzbHySPQDAFM4ke3fG7D0YTDmjGx8AgABHZQ8AMAVm4wMAEOAMufdNej/uxacbHwCAQEdlDwAwBbrxAQAIdCbuxyfZAwDMwc3KXn5c2TNmDwBAgKOyBwCYAm/QAwAgwJl5gh7d+AAABDgqewCAORgW9ybZ+XFlT7IHAJiCmcfs6cYHACDAUdkDAMyBl+pc2Mcff1zmE950002XHAwAAN5i5tn4ZUr2N998c5lOZrFYVFxc7E48AADAw8qU7O12u7fjAADA+/y4K94dbo3Z5+fnKyQkxFOxAADgNWbuxnd5Nn5xcbGeeuopXXbZZapWrZr27t0rSRo/frzmzZvn8QABAPAIwwOLn3I52T/zzDNasGCBpk6dquDgYEd7y5Yt9cYbb3g0OAAA4D6Xk/1bb72l1157TYMGDVKlSpUc7W3atNGuXbs8GhwAAJ5j8cDin1wes//ll1/UuHHjUu12u12FhYUeCQoAAI8z8XP2Llf2LVq00MaNG0u1//vf/1a7du08EhQAAPAclyv7CRMmKCkpSb/88ovsdrs++OAD7d69W2+99ZZWrFjhjRgBAHAflX3Z9evXT8uXL9fq1atVtWpVTZgwQTt37tTy5ct1/fXXeyNGAADcV/LVO3cWP3VJz9lfd911WrVqladjAQAAXnDJL9XZunWrdu7cKenMOH6HDh08FhQAAJ5m5k/cupzsf/75Zw0cOFD/+c9/FBkZKUk6fvy4rrnmGi1dulR169b1dIwAALiPMfuyGzZsmAoLC7Vz505lZ2crOztbO3fulN1u17Bhw7wRIwAAfmfDhg3q27evYmNjZbFYtGzZMqfthmFowoQJiomJUWhoqHr06KE9e/Y47ZOdna1BgwbJZrMpMjJSQ4cOVW5ursuxuJzs169frzlz5qhp06aOtqZNm+qll17Shg0bXA4AAIByUc4T9PLy8tSmTRvNnj37nNunTp2qWbNmae7cudqyZYuqVq2qXr16KT8/37HPoEGD9MMPP2jVqlVasWKFNmzYoBEjRrh86y5348fFxZ3z5TnFxcWKjY11OQAAAMqDxTizuHO8KxITE5WYmHjObYZhaObMmXryySfVr18/SWfeUFunTh0tW7ZMAwYM0M6dO7Vy5Up9/fXXuvLKKyVJL730kvr06aPnn3/epZzrcmU/bdo0Pfjgg9q6daujbevWrXr44Yf1/PPPu3o6AADKh4c+hJOTk+O0FBQUuBxKRkaGsrKy1KNHD0dbRESEEhISlJKSIklKSUlRZGSkI9FLUo8ePRQUFKQtW7a4dL0yVfbVq1eXxfK/7ou8vDwlJCSocuUzhxcVFaly5cq65557dPPNN7sUAAAA/iQuLs5pfeLEiUpOTnbpHFlZWZKkOnXqOLXXqVPHsS0rK0u1a9d22l65cmVFRUU59imrMiX7mTNnunRSAAAqHHdfjPPHsQcPHpTNZnM0W61WdyPzujIl+6SkJG/HAQCAd3no0TubzeaU7C9FdHS0JOnw4cOKiYlxtB8+fFht27Z17HPkyBGn44qKipSdne04vqxcHrP/s/z8/FJjFwAA4MIaNGig6OhorVmzxtGWk5OjLVu2qGPHjpKkjh076vjx49q2bZtjn7Vr18putyshIcGl67k8Gz8vL09jx47Vu+++q2PHjpXaXlxc7OopAQDwvnJ+qU5ubq7S0tIc6xkZGUpNTVVUVJTq1aunf/zjH3r66ad1+eWXq0GDBho/frxiY2Mdc9+aN2+u3r17a/jw4Zo7d64KCws1cuRIDRgwwOWn31yu7B977DGtXbtWc+bMkdVq1RtvvKFJkyYpNjZWb731lqunAwCgfHhoNn5Zbd26Ve3atXN8/n306NFq166dJkyYIOlMPn3wwQc1YsQIXXXVVcrNzdXKlSsVEhLiOMfixYvVrFkzde/eXX369NG1116r1157zeVbtxiGa2/7rVevnt566y117dpVNptN27dvV+PGjbVo0SK9/fbb+vTTT10O4lLl5OQoIiJCv/3UULZwt0YkgAqrV2xbX4cAeE2RUah1+kgnTpxwexz8fEpyRdzzTykoNOTiB5yH/fd8HRwz3quxeovLGTI7O1sNGzaUdGaSQnZ2tiTp2muv5Q16AICKy8SfuHU52Tds2FAZGRmSpGbNmundd9+VJC1fvtzxYRwAACqakjfoubP4K5eT/ZAhQ/Ttt99Kkh5//HHNnj1bISEhGjVqlB599FGPBwgAANzj8mz8UaNGOf7co0cP7dq1S9u2bVPjxo3VunVrjwYHAIDHmPgTty4n+7PFx8crPj7eE7EAAAAvKFOynzVrVplP+NBDD11yMAAAeItFbn71zmORlL8yJfsZM2aU6WQWi4VkDwBABVOmZF8y+76i+mvX61U5qOJ/iAC4NId8HQAQGDz0IRx/5PaYPQAAfsHEE/R47RwAAAGOyh4AYA4mruxJ9gAAU3D3LXimeoMeAADwL5eU7Ddu3Ki77rpLHTt21C+//CJJWrRokTZt2uTR4AAA8Jhy/sRtReJysn///ffVq1cvhYaG6ptvvlFBQYEk6cSJE3r22Wc9HiAAAB5Bsi+7p59+WnPnztXrr7+uKlWqONo7deqk7du3ezQ4AADgPpcn6O3evVudO3cu1R4REaHjx497IiYAADyOCXouiI6OVlpaWqn2TZs2qWHDhh4JCgAAjyt5g547i59yOdkPHz5cDz/8sLZs2SKLxaJDhw5p8eLFGjNmjP7+9797I0YAANxn4jF7l7vxH3/8cdntdnXv3l2nTp1S586dZbVaNWbMGD344IPeiBEAALjB5WRvsVj0xBNP6NFHH1VaWppyc3PVokULVatWzRvxAQDgEWYes7/kN+gFBwerRYsWnowFAADv4XW5ZdetWzdZLOefpLB27Vq3AgIAAJ7lcrJv27at03phYaFSU1P1/fffKykpyVNxAQDgWW5245uqsp8xY8Y525OTk5Wbm+t2QAAAeIWJu/E99iGcu+66S2+++aanTgcAADzEY5+4TUlJUUhIiKdOBwCAZ5m4snc52ffv399p3TAMZWZmauvWrRo/frzHAgMAwJN49M4FERERTutBQUFq2rSpJk+erJ49e3osMAAA4BkuJfvi4mINGTJErVq1UvXq1b0VEwAA8CCXJuhVqlRJPXv25Ot2AAD/Y+J347s8G79ly5bau3evN2IBAMBrSsbs3Vn8lcvJ/umnn9aYMWO0YsUKZWZmKicnx2kBAAAVS5nH7CdPnqxHHnlEffr0kSTddNNNTq/NNQxDFotFxcXFno8SAABP8OPq3B1lTvaTJk3Sfffdpy+//NKb8QAA4B08Z39xhnHmLrt06eK1YAAAgOe59Ojdhb52BwBARcZLdcqoSZMmF0342dnZbgUEAIBX0I1fNpMmTSr1Bj0AAFBa/fr1tX///lLt999/v2bPnq2uXbtq/fr1TtvuvfdezZ071+OxuJTsBwwYoNq1a3s8CAAAvK28u/G//vprpyfUvv/+e11//fX661//6mgbPny4Jk+e7FgPCwu79AAvoMzJnvF6AIBfK+du/Fq1ajmtP/fcc2rUqJHTRPewsDBFR0e7EVTZlPmlOiWz8QEAgGtOnz6tf/3rX7rnnnuciufFixerZs2aatmypcaNG6dTp0555fplruztdrtXAgAAoFx4qLI/+22xVqtVVqv1gocuW7ZMx48f1+DBgx1td955p+Lj4xUbG6sdO3Zo7Nix2r17tz744AM3gjw3lz9xCwCAP/LUmH1cXJxT+8SJE5WcnHzBY+fNm6fExETFxsY62kaMGOH4c6tWrRQTE6Pu3bsrPT1djRo1uvRAz4FkDwAwBw9V9gcPHpTNZnM0X6yq379/v1avXn3Rij0hIUGSlJaWRrIHAMCXbDabU7K/mPnz56t27dq64YYbLrhfamqqJCkmJsad8M6JZA8AMAcfvFTHbrdr/vz5SkpKUuXK/0u56enpWrJkifr06aMaNWpox44dGjVqlDp37qzWrVu7EeS5kewBAKbgi9flrl69WgcOHNA999zj1B4cHKzVq1dr5syZysvLU1xcnG699VY9+eSTlx7gBZDsAQDwkp49e57z0fW4uLhSb8/zJpI9AMAceDc+AACBzcxfvSvzG/QAAIB/orIHAJgD3fgAAAQ4Eyd7uvEBAAhwVPYAAFOw/LG4c7y/ItkDAMzBxN34JHsAgCnw6B0AAAhYVPYAAHOgGx8AABPw44TtDrrxAQAIcFT2AABTMPMEPZI9AMAcTDxmTzc+AAABjsoeAGAKdOMDABDo6MYHAACBisoeAGAKdOMDABDoTNyNT7IHAJiDiZM9Y/YAAAQ4KnsAgCkwZg8AQKCjGx8AAAQqKnsAgClYDEMW49LLc3eO9TWSPQDAHOjGBwAAgYrKHgBgCszGBwAg0NGNDwAAAhWVPQDAFOjGBwAg0Jm4G59kDwAwBTNX9ozZAwAQ4KjsAQDmQDc+AACBz5+74t1BNz4AAAGOZA8AMAfDcH9xQXJysiwWi9PSrFkzx/b8/Hw98MADqlGjhqpVq6Zbb71Vhw8f9vRdSyLZAwBMomQ2vjuLq6644gplZmY6lk2bNjm2jRo1SsuXL9d7772n9evX69ChQ+rfv78H7/h/GLMHAMBLKleurOjo6FLtJ06c0Lx587RkyRL93//9nyRp/vz5at68ub766iv95S9/8WgcVPYAAHMwPLBIysnJcVoKCgrOe8k9e/YoNjZWDRs21KBBg3TgwAFJ0rZt21RYWKgePXo49m3WrJnq1aunlJQUj962RLIHAJiExe7+IklxcXGKiIhwLFOmTDnn9RISErRgwQKtXLlSc+bMUUZGhq677jqdPHlSWVlZCg4OVmRkpNMxderUUVZWlsfvnW58AABccPDgQdlsNse61Wo9536JiYmOP7du3VoJCQmKj4/Xu+++q9DQUK/H+WdU9ijlinbZmjB9q976dK0++foz/aXL+WeHPvD49/rk68/Ub2BGOUYIeEffwb9q4ZYftXzvDr24Yo+atj3l65DgSR7qxrfZbE7L+ZL92SIjI9WkSROlpaUpOjpap0+f1vHjx532OXz48DnH+N3l02S/YcMG9e3bV7GxsbJYLFq2bJkvw8EfQkKLlfGTTXOmtrjgfh27ZqlZq+P69UjZ/qIDFVmXm37TiImHtHh6tB7o1UR7fwzRM0v2KqJGoa9Dg4f4Yjb+n+Xm5io9PV0xMTHq0KGDqlSpojVr1ji27969WwcOHFDHjh3dvNPSfJrs8/Ly1KZNG82ePduXYeAs2zbX0qK5TZSy7vy/Xdaola/7xvyoaePbqLiIDiL4v/4jftXKJVH64p0oHdgTollj66rgd4t6Dcz2dWjwlHJ+zn7MmDFav3699u3bp82bN+uWW25RpUqVNHDgQEVERGjo0KEaPXq0vvzyS23btk1DhgxRx44dPT4TX/LxmH1iYqLTmAb8g8Vi6JFJ3+r9fzXUgb3hvg4HcFvlKnZd3vqUlr5c29FmGBZ9szFcLTrQlY9L8/PPP2vgwIE6duyYatWqpWuvvVZfffWVatWqJUmaMWOGgoKCdOutt6qgoEC9evXSK6+84pVY/GqCXkFBgdMjDjk5OT6MxrxuS9qr4mKLPl4a7+tQAI+wRRWrUmXp+FHnfxJ/+7Wy4hqf/7Eq+Jfy/sTt0qVLL7g9JCREs2fPLpfebb/qf50yZYrT4w5xcXG+Dsl0Gjc7oX4D9mnGpNaSLL4OBwDKzkMT9PyRX1X248aN0+jRox3rOTk5JPxydkW7bEVUP60Fy9c52ipVNjT04V3qN2C/7unX1VehAZcsJ7uSioukyFpFTu3Vaxbpt6N+9c8kcE5+9bfYarWW+REHeMfaTy9T6n9rOrVNnvW1vvzsMq1afpmPogLcU1QYpD07wtTu2pNKWRkh6czclLbX5urjBTV8HB08pby78SsSv0r2KB8hoUWKjfvfpKTo2FNq2CRHJ09U0dHDoTp5Ithp/+KiIP12LFi/7K9W3qECHvPBazU1ZuZB/fRtmHZ/E6Zbhh9VSJhdXyyN8nVo8JRLmFFf6ng/5dNkn5ubq7S0NMd6RkaGUlNTFRUVpXr16vkwMnO7vPkJPffqfx3rw0fvkiStXnHZH2P1QOBZ/3F1RdQo1t2PZql6rSLt/SFUTwxqoOO/VvF1aIDbfJrst27dqm7dujnWS8bjk5KStGDBAh9Fhe+219ANV5X9kUjG6REoPp5fUx/Pr3nxHeGX6Mb3ka5du8rw424RAIAfcXdGvR+nK7969A4AALiOCXoAAFOgGx8AgEBnN84s7hzvp0j2AABzYMweAAAEKip7AIApWOTmmL3HIil/JHsAgDmY+A16dOMDABDgqOwBAKbAo3cAAAQ6ZuMDAIBARWUPADAFi2HI4sYkO3eO9TWSPQDAHOx/LO4c76foxgcAIMBR2QMATIFufAAAAp2JZ+OT7AEA5sAb9AAAQKCisgcAmAJv0AMAINDRjQ8AAAIVlT0AwBQs9jOLO8f7K5I9AMAc6MYHAACBisoeAGAOvFQHAIDAZubX5dKNDwBAgKOyBwCYg4kn6JHsAQDmYMi9b9L7b64n2QMAzIExewAAELCo7AEA5mDIzTF7j0VS7qjsAQDmUDJBz53FBVOmTNFVV12l8PBw1a5dWzfffLN2797ttE/Xrl1lsViclvvuu8+Tdy2JZA8AgFesX79eDzzwgL766iutWrVKhYWF6tmzp/Ly8pz2Gz58uDIzMx3L1KlTPR4L3fgAAHOwS7K4ebwLVq5c6bS+YMEC1a5dW9u2bVPnzp0d7WFhYYqOjnYjsIujsgcAmELJbHx3FknKyclxWgoKCsp0/RMnTkiSoqKinNoXL16smjVrqmXLlho3bpxOnTrl2RsXlT0AAC6Ji4tzWp84caKSk5MveIzdbtc//vEPderUSS1btnS033nnnYqPj1dsbKx27NihsWPHavfu3frggw88GjPJHgBgDh56g97Bgwdls9kczVar9aKHPvDAA/r++++1adMmp/YRI0Y4/tyqVSvFxMSoe/fuSk9PV6NGjS491rOQ7AEA5uChZG+z2ZyS/cWMHDlSK1as0IYNG1S3bt0L7puQkCBJSktLI9kDAFDRGYahBx98UB9++KHWrVunBg0aXPSY1NRUSVJMTIxHYyHZAwDMoZw/hPPAAw9oyZIl+uijjxQeHq6srCxJUkREhEJDQ5Wenq4lS5aoT58+qlGjhnbs2KFRo0apc+fOat269aXHeQ4kewCAOZTzo3dz5syRdObFOX82f/58DR48WMHBwVq9erVmzpypvLw8xcXF6dZbb9WTTz7pRpDnRrIHAJhCeX8Ix7jI/nFxcVq/fv0lx+MKnrMHACDAUdkDAMyhnMfsKxKSPQDAHOyGZHEjYdv9N9nTjQ8AQICjsgcAmAPd+AAABDo3k738N9nTjQ8AQICjsgcAmAPd+AAABDi7Ibe64pmNDwAAKioqewCAORj2M4s7x/spkj0AwBwYswcAIMAxZg8AAAIVlT0AwBzoxgcAIMAZcjPZeyySckc3PgAAAY7KHgBgDnTjAwAQ4Ox2SW48K2/33+fs6cYHACDAUdkDAMyBbnwAAAKciZM93fgAAAQ4KnsAgDmY+HW5JHsAgCkYhl2GG1+uc+dYXyPZAwDMwTDcq84ZswcAABUVlT0AwBwMN8fs/biyJ9kDAMzBbpcsboy7+/GYPd34AAAEOCp7AIA50I0PAEBgM+x2GW504/vzo3d04wMAEOCo7AEA5kA3PgAAAc5uSBZzJnu68QEACHBU9gAAczAMSe48Z++/lT3JHgBgCobdkOFGN77hx8mebnwAgDkYdveXSzB79mzVr19fISEhSkhI0H//+18P39jFkewBAPCSd955R6NHj9bEiRO1fft2tWnTRr169dKRI0fKNQ6SPQDAFAy74fbiqunTp2v48OEaMmSIWrRooblz5yosLExvvvmmF+7w/Ej2AABzKOdu/NOnT2vbtm3q0aOHoy0oKEg9evRQSkqKp+/ugvx6gl7JZIki+2kfRwJ4T5FR6OsQAK8p0pm/3+Ux+a1IhW69U6ck1pycHKd2q9Uqq9Vaav9ff/1VxcXFqlOnjlN7nTp1tGvXrksP5BL4dbI/efKkJGld1nwfRwIAcMfJkycVERHhlXMHBwcrOjpam7I+dftc1apVU1xcnFPbxIkTlZyc7Pa5vcmvk31sbKwOHjyo8PBwWSwWX4djCjk5OYqLi9PBgwdls9l8HQ7gUfz9Ln+GYejkyZOKjY312jVCQkKUkZGh06fd7wU2DKNUvjlXVS9JNWvWVKVKlXT48GGn9sOHDys6OtrtWFzh18k+KChIdevW9XUYpmSz2fjHEAGLv9/ly1sV/Z+FhIQoJCTE69f5s+DgYHXo0EFr1qzRzTffLEmy2+1as2aNRo4cWa6x+HWyBwCgIhs9erSSkpJ05ZVX6uqrr9bMmTOVl5enIUOGlGscJHsAALzkjjvu0NGjRzVhwgRlZWWpbdu2WrlyZalJe95GsodLrFarJk6ceN4xKsCf8fcb3jBy5Mhy77Y/m8Xw55f9AgCAi+KlOgAABDiSPQAAAY5kDwBAgCPZAwAQ4Ej2KLOK8E1mwBs2bNigvn37KjY2VhaLRcuWLfN1SIBHkexRJhXlm8yAN+Tl5alNmzaaPXu2r0MBvIJH71AmCQkJuuqqq/Tyyy9LOvPKx7i4OD344IN6/PHHfRwd4DkWi0Uffvih4/WmQCCgssdFVaRvMgMAXEeyx0Vd6JvMWVlZPooKAFBWJHsAAAIcyR4XVZG+yQwAcB3JHhf1528ylyj5JnPHjh19GBkAoCz46h3KpKJ8kxnwhtzcXKWlpTnWMzIylJqaqqioKNWrV8+HkQGewaN3KLOXX35Z06ZNc3yTedasWUpISPB1WIDb1q1bp27dupVqT0pK0oIFC8o/IMDDSPYAAAQ4xuwBAAhwJHsAAAIcyR4AgABHsgcAIMCR7AEACHAkewAAAhzJHgCAAEeyB9w0ePBgp2+fd+3aVf/4xz/KPY5169bJYrHo+PHj593HYrFo2bJlZT5ncnKy2rZt61Zc+/btk8ViUWpqqlvnAXDpSPYISIMHD5bFYpHFYlFwcLAaN26syZMnq6ioyOvX/uCDD/TUU0+Vad+yJGgAcBfvxkfA6t27t+bPn6+CggJ9+umneuCBB1SlShWNGzeu1L6nT59WcHCwR64bFRXlkfMAgKdQ2SNgWa1WRUdHKz4+Xn//+9/Vo0cPffzxx5L+1/X+zDPPKDY2Vk2bNpUkHTx4ULfffrsiIyMVFRWlfv36ad++fY5zFhcXa/To0YqMjFSNGjX02GOP6ew3Tp/djV9QUKCxY8cqLi5OVqtVjRs31rx587Rv3z7H+9irV68ui8WiwYMHSzrzVcEpU6aoQYMGCg0NVZs2bfTvf//b6TqffvqpmjRpotDQUHXr1s0pzrIaO3asmjRporCwMDVs2FDjx49XYWFhqf1effVVxcXFKSwsTLfffrtOnDjhtP2NN95Q8+bNFRISombNmumVV15xORYA3kOyh2mEhobq9OnTjvU1a9Zo9+7dWrVqlVasWKHCwkL16tVL4eHh2rhxo/7zn/+oWrVq6t27t+O4F154QQsWLNCbb76pTZs2KTs7Wx9++OEFr3v33Xfr7bff1qxZs7Rz5069+uqrqlatmuLi4vT+++9Lknbv3q3MzEy9+OKLkqQpU6borbfe0ty5c/XDDz9o1KhRuuuuu7R+/XpJZ34p6d+/v/r27avU1FQNGzZMjz/+uMs/k/DwcC1YsEA//vijXnzxRb3++uuaMWOG0z5paWl69913tXz5cq1cuVLffPON7r//fsf2xYsXa8KECXrmmWe0c+dOPfvssxo/frwWLlzocjwAvMQAAlBSUpLRr18/wzAMw263G6tWrTKsVqsxZswYx/Y6deoYBQUFjmMWLVpkNG3a1LDb7Y62goICIzQ01Pj8888NwzCMmJgYY+rUqY7thYWFRt26dR3XMgzD6NKli/Hwww8bhmEYu3fvNiQZq1atOmecX375pSHJ+O233xxt+fn5RlhYmLF582anfYcOHWoMHDjQMAzDGDdunNGiRQun7WPHji11rrNJMj788MPzbp82bZrRoUMHx/rEiRONSpUqGT///LOj7bPPPjOCgoKMzMxMwzAMo1GjRsaSJUuczvPUU08ZHTt2NAzDMDIyMgxJxjfffHPe6wLwLsbsEbBWrFihatWqqbCwUHa7XXfeeaeSk5Md21u1auU0Tv/tt98qLS1N4eHhTufJz89Xenq6Tpw4oczMTKfP+lauXFlXXnllqa78EqmpqapUqZK6dOlS5rjT0tJ06tQpXX/99U7tp0+fVrt27SRJO3fuLPV54Y4dO5b5GiXeeecdzZo1S+np6crNzVVRUZFsNpvTPvXq1dNll13mdB273a7du3crPDxc6enpGjp0qIYPH+7Yp6ioSBERES7HA8A7SPYIWN26ddOcOXMUHBys2NhYVa7s/Ne9atWqTuu5ubnq0KGDFi9eXOpctWrVuqQYQkNDXT4mNzdXkvTJJ584JVnpzDwET0lJSdGgQYM0adIk9erVSxEREVq6dKleeOEFl2N9/fXXS/3yUalSJY/FCsA9JHsErKpVq6px48Zl3r99+/Z65513VLt27VLVbYmYmBht2bJFnTt3lnSmgt22bZvat29/zv1btWolu92u9evXq0ePHqW2l/QsFBcXO9patGghq9WqAwcOnLdHoHnz5o7JhiW++uqri9/kn2zevFnx8fF64oknHG379+8vtd+BAwd06NAhxcbGOq4TFBSkpk2bqk6dOoqNjdXevXs1aNAgl64PoPwwQQ/4w6BBg1SzZk3169dPGzduVEZGhtatW6eHHnpIP//8syTp4Ycf1nPPPadly5Zp165duv/++y/4jHz9+vWVlJSke+65R8uWLXOc891335UkxcfHy2KxaMWKFTp69Khyc3MVHh6uMWPGaNSoUVq4cKHS09O1fft2vfTSS45Jb/fdd5/27NmjRx99VLt379aSJUu0YMECl+738ssv14EDB7R06VKlp6dr1qxZ55xsGBISoqSkJH377bfauHGjHnroId1+++2Kjo6WJE2aNElTpkzRrFmz9NNPP+m7777T/PnzNX36dJfiAeA9JHvgD2FhYdqwYYPq1aun/v37q3nz5ho6dKjy8/Mdlf4jjzyiv/3tb0pKSlLHjh0VHh6uW2655YLnnTNnjm677Tbdf//9atasmYYPH668vDxJ0mWXXaZJkybp8ccfV506dTRy5EhJ0lNPPaXx48drypQpat68uXr37q1PPvlEDRo0kHRmHP3999/XsmXL1KZNG82dO1fPPvusS/d70003adSoURo5cqTatm2rzZs3a/z48aX2a9y4sfr3768+ffqoZ8+eat26tdOjdcOGDdMbb7yh+fPnq1WrVurSpYsWLFjgiBWA71mM880sAgAAAYHKHgCAAEeyBwAgwJHsAQAIcCR7AAACHMkeAIAAR7IHACDAkewBAAhwJHsAAAIcyR4AgABHsgcAIMCR7AEACHAkewAAAtz/B0SK9ikEbk/qAAAAAElFTkSuQmCC\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#12. Write a Python program to train a Logistic Regression model and evaluate its performance using Precision, Recall, and F1-Score.\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score\n",
        "\n",
        "print(\"Precision:\", precision_score(y_test, y_pred))\n",
        "print(\"Recall:\", recall_score(y_test, y_pred))\n",
        "print(\"F1-Score:\", f1_score(y_test, y_pred))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "88_I_l7_nm4_",
        "outputId": "f0a9c7d7-f5ec-4687-c7e8-9c670dc40a6f"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Precision: 0.0\n",
            "Recall: 0.0\n",
            "F1-Score: 0.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#13. Write a Python program to train a Logistic Regression model on imbalanced data and apply class weights to improve model performance.\n",
        "from sklearn.datasets import make_classification\n",
        "\n",
        "# Simulate imbalanced data\n",
        "X_imb, y_imb = make_classification(n_samples=1000, n_classes=2,\n",
        "                                   weights=[0.9, 0.1], flip_y=0,\n",
        "                                   n_features=20, random_state=42)\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_imb, y_imb, test_size=0.2, random_state=42)\n",
        "\n",
        "model = LogisticRegression(class_weight='balanced', max_iter=1000)\n",
        "model.fit(X_train, y_train)\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "print(\"Accuracy with Class Weights:\", accuracy_score(y_test, y_pred))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p7mFdxJ4nm0v",
        "outputId": "8bbcaab5-f3ce-4543-dff5-b84294604ffb"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy with Class Weights: 0.825\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#14. Write a Python program to train Logistic Regression on the Titanic dataset, handle missing values, and evaluate performance.\n",
        "import seaborn as sns\n",
        "\n",
        "# Load dataset\n",
        "titanic = sns.load_dataset(\"titanic\")\n",
        "titanic = titanic[['sex', 'age', 'fare', 'survived']].dropna()\n",
        "\n",
        "# Convert categorical column\n",
        "titanic['sex'] = titanic['sex'].map({'male': 0, 'female': 1})\n",
        "\n",
        "X = titanic[['sex', 'age', 'fare']]\n",
        "y = titanic['survived']\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "model = LogisticRegression(max_iter=1000)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "print(\"Titanic Dataset Accuracy:\", model.score(X_test, y_test))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VRuLe3ncnmxR",
        "outputId": "15cbdab6-56e8-4ead-e18a-db9895ebd3e9"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Titanic Dataset Accuracy: 0.7482517482517482\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#15. Write a Python program to apply feature scaling (Standardization) before training a Logistic Regression model. Evaluate its accuracy and compare results with and without scaling.\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Without scaling\n",
        "model = LogisticRegression(max_iter=1000)\n",
        "model.fit(X_train, y_train)\n",
        "print(\"Without Scaling Accuracy:\", model.score(X_test, y_test))\n",
        "\n",
        "# With Standardization\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "model_scaled = LogisticRegression(max_iter=1000)\n",
        "model_scaled.fit(X_train_scaled, y_train)\n",
        "print(\"With Scaling Accuracy:\", model_scaled.score(X_test_scaled, y_test))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iiuNam1vnmtg",
        "outputId": "95475f4e-306f-406a-c2b4-f144df9e458c"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Without Scaling Accuracy: 0.7482517482517482\n",
            "With Scaling Accuracy: 0.7482517482517482\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#16. Write a Python program to train Logistic Regression and evaluate its performance using ROC-AUC score.\n",
        "from sklearn.metrics import roc_auc_score\n",
        "\n",
        "model = LogisticRegression(max_iter=1000)\n",
        "model.fit(X_train, y_train)\n",
        "y_proba = model.predict_proba(X_test)[:, 1]\n",
        "\n",
        "print(\"ROC-AUC Score:\", roc_auc_score(y_test, y_proba))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "19A2ingYnmpw",
        "outputId": "250e0b5e-7521-4eb6-9fe5-e3dc813ea780"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ROC-AUC Score: 0.7475369458128078\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#17. Write a Python program to train Logistic Regression using a custom learning rate (C=0.5) and evaluate accuracy.\n",
        "model = LogisticRegression(C=0.5, max_iter=1000)\n",
        "model.fit(X_train, y_train)\n",
        "print(\"Accuracy with C=0.5:\", model.score(X_test, y_test))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fzv1tzM4nmmG",
        "outputId": "7b19087a-b5e2-4c92-fbb0-4a4c929c8f84"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy with C=0.5: 0.7482517482517482\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#18. Write a Python program to train Logistic Regression and identify important features based on model coefficients.\n",
        "import numpy as np\n",
        "\n",
        "model = LogisticRegression(max_iter=1000)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "feature_names = X_train.columns if hasattr(X_train, 'columns') else [f\"Feature {i}\" for i in range(X_train.shape[1])]\n",
        "importance = model.coef_[0]\n",
        "\n",
        "for name, coef in sorted(zip(feature_names, importance), key=lambda x: abs(x[1]), reverse=True):\n",
        "    print(f\"{name}: {coef:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i7tzdNDQnmib",
        "outputId": "2b4f0759-0bcc-45a2-a549-0603da7f6ae4"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "sex: 2.3703\n",
            "age: -0.0184\n",
            "fare: 0.0128\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#19. Write a Python program to train Logistic Regression and evaluate its performance using Cohen‚Äôs Kappa Score.\n",
        "from sklearn.metrics import cohen_kappa_score\n",
        "\n",
        "model = LogisticRegression(max_iter=1000)\n",
        "model.fit(X_train, y_train)\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "print(\"Cohen's Kappa Score:\", cohen_kappa_score(y_test, y_pred))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "txqoQc1Wnmen",
        "outputId": "a800b02f-304d-4bb1-8d8f-2d99aa795767"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cohen's Kappa Score: 0.4716748768472906\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#20. Write a Python program to train Logistic Regression and visualize the Precision-Recall Curve for binary classification.\n",
        "from sklearn.metrics import precision_recall_curve, PrecisionRecallDisplay\n",
        "\n",
        "model = LogisticRegression(max_iter=1000)\n",
        "model.fit(X_train, y_train)\n",
        "y_proba = model.predict_proba(X_test)[:, 1]\n",
        "\n",
        "precision, recall, _ = precision_recall_curve(y_test, y_proba)\n",
        "disp = PrecisionRecallDisplay(precision=precision, recall=recall)\n",
        "disp.plot()\n",
        "plt.title(\"Precision-Recall Curve\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 472
        },
        "id": "vYuEA2iNnma3",
        "outputId": "fea590a5-39b2-4a44-c334-76b25c29ee46"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#21. Write a Python program to train Logistic Regression with different solvers (liblinear, saga, lbfgs) and compare their accuracy.\n",
        "solvers = ['liblinear', 'saga', 'lbfgs']\n",
        "\n",
        "for solver in solvers:\n",
        "    model = LogisticRegression(solver=solver, max_iter=1000)\n",
        "    model.fit(X_train, y_train)\n",
        "    score = model.score(X_test, y_test)\n",
        "    print(f\"Solver: {solver}, Accuracy: {score}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FbYZ0dgsnmW5",
        "outputId": "5cfc110e-78ab-4f48-fb81-12820d7cdfe0"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Solver: liblinear, Accuracy: 0.7482517482517482\n",
            "Solver: saga, Accuracy: 0.6293706293706294\n",
            "Solver: lbfgs, Accuracy: 0.7482517482517482\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#22. Write a Python program to train Logistic Regression and evaluate its performance using Matthews Correlation Coefficient (MCC).\n",
        "from sklearn.metrics import matthews_corrcoef\n",
        "\n",
        "model = LogisticRegression(max_iter=1000)\n",
        "model.fit(X_train, y_train)\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "print(\"Matthews Correlation Coefficient (MCC):\", matthews_corrcoef(y_test, y_pred))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zNa4Xl3mnmTG",
        "outputId": "07ad9d51-3580-4cfd-9615-a09183be6729"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Matthews Correlation Coefficient (MCC): 0.47167487684729065\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#23. Write a Python program to train Logistic Regression on both raw and standardized data. Compare their accuracy to see the impact of feature scaling.\n",
        "# Raw data\n",
        "model_raw = LogisticRegression(max_iter=1000)\n",
        "model_raw.fit(X_train, y_train)\n",
        "raw_acc = model_raw.score(X_test, y_test)\n",
        "\n",
        "# Standardized\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "model_scaled = LogisticRegression(max_iter=1000)\n",
        "model_scaled.fit(X_train_scaled, y_train)\n",
        "scaled_acc = model_scaled.score(X_test_scaled, y_test)\n",
        "\n",
        "print(\"Raw Data Accuracy:\", raw_acc)\n",
        "print(\"Standardized Data Accuracy:\", scaled_acc)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2opLphnQnmOW",
        "outputId": "8a9270bf-3021-4b6f-f9e1-5fb58a9e9582"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Raw Data Accuracy: 0.7482517482517482\n",
            "Standardized Data Accuracy: 0.7482517482517482\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#24. Write a Python program to train Logistic Regression and find the optimal C (regularization strength) using cross-validation.\n",
        "from sklearn.model_selection import cross_val_score\n",
        "\n",
        "C_values = [0.01, 0.1, 1, 10, 100]\n",
        "for c in C_values:\n",
        "    model = LogisticRegression(C=c, max_iter=1000)\n",
        "    scores = cross_val_score(model, X, y, cv=5)\n",
        "    print(f\"C={c}, Mean Accuracy: {scores.mean():.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lOp5K8KSnmI3",
        "outputId": "d37fa075-505a-4442-d69c-c41d4f39d72a"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "C=0.01, Mean Accuracy: 0.6933\n",
            "C=0.1, Mean Accuracy: 0.7732\n",
            "C=1, Mean Accuracy: 0.7745\n",
            "C=10, Mean Accuracy: 0.7745\n",
            "C=100, Mean Accuracy: 0.7745\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#25. Write a Python program to train Logistic Regression, save the trained model using joblib, and load it again to make predictions.\n",
        "import joblib\n",
        "\n",
        "# Train and save\n",
        "model = LogisticRegression(max_iter=1000)\n",
        "model.fit(X_train, y_train)\n",
        "joblib.dump(model, 'logreg_model.pkl')\n",
        "\n",
        "# Load and predict\n",
        "loaded_model = joblib.load('logreg_model.pkl')\n",
        "y_pred = loaded_model.predict(X_test)\n",
        "print(\"Loaded Model Accuracy:\", accuracy_score(y_test, y_pred))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9XtNHLbjnl9f",
        "outputId": "169344a5-404c-4a5d-cc34-4396be997c84"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded Model Accuracy: 0.7482517482517482\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "UeSTDkoSq0Sq"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}